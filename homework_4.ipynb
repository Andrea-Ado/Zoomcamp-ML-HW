{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59d0f512-bf1e-4d12-b41a-43f97a7b2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mutual_info_score, accuracy_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "480879cb-03ca-42eb-b611-f9771752f5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "\n",
    "df = pd.read_csv('data/course_lead_scoring.csv')\n",
    "\n",
    "# Fill missing values\n",
    "for column in df.columns:\n",
    "    if df[column].dtype == 'object':\n",
    "        df[column] = df[column].fillna('NA')\n",
    "    else:\n",
    "        df[column] = df[column].fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af98c5b-15d0-4b3d-ab24-bd8b77c6ee1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 876\n",
      "Validation set size: 293\n",
      "Test set size: 293\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split the data\n",
    "\n",
    "X = df.drop(columns=['converted'])\n",
    "y = df['converted']\n",
    "\n",
    "# 80% train+val, 20% test\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1\n",
    ")\n",
    "\n",
    "# 60% train, 20% val\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.25, random_state=1\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]}\")\n",
    "print(f\"Validation set size: {X_val.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf267e1-5622-4b96-8399-6c12a6556112",
   "metadata": {},
   "source": [
    "### Question 1: ROC AUC feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9d65b7f-0fcb-4ef0-a771-336a0024b57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_of_courses_viewed: 0.7636\n",
      "annual_income: 0.5520\n",
      "interaction_count: 0.7383\n",
      "lead_score: 0.6145\n",
      "\n",
      "Variable with the highest AUC: number_of_courses_viewed (0.7636)\n"
     ]
    }
   ],
   "source": [
    "# Select numerical variables\n",
    "numerical_vars = X_train.select_dtypes(include='number').columns.tolist()\n",
    "\n",
    "auc_scores = {}\n",
    "\n",
    "for var in numerical_vars:\n",
    "    auc = roc_auc_score(y_train, X_train[var])\n",
    "    if auc < 0.5:\n",
    "        auc = roc_auc_score(y_train, -X_train[var])\n",
    "    auc_scores[var] = auc\n",
    "\n",
    "# Print all AUCs\n",
    "for var, auc in auc_scores.items():\n",
    "    print(f\"{var}: {auc:.4f}\")\n",
    "\n",
    "# Find variable with highest AUC\n",
    "best_var = max(auc_scores, key=auc_scores.get)\n",
    "print(f\"\\nVariable with the highest AUC: {best_var} ({auc_scores[best_var]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694af7f6-88fc-4e02-be61-5fe03a7b2266",
   "metadata": {},
   "source": [
    "### Question 2: Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eabb975-e2ff-4ab4-9687-4b05df074a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation AUC: 0.920\n"
     ]
    }
   ],
   "source": [
    "# Define categorical and numerical columns\n",
    "categorical = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Convert categorical columns to string\n",
    "for col in categorical:\n",
    "    X_train[col] = X_train[col].astype(str)\n",
    "    X_val[col] = X_val[col].astype(str)\n",
    "\n",
    "# Combine categorical and numerical features\n",
    "features = categorical + numerical\n",
    "\n",
    "# Convert data to dictionary format\n",
    "train_dict = X_train[features].to_dict(orient='records')\n",
    "val_dict = X_val[features].to_dict(orient='records')\n",
    "\n",
    "# One-hot encode with DictVectorizer\n",
    "dv = DictVectorizer(sparse=False)\n",
    "X_train_encoded = dv.fit_transform(train_dict)\n",
    "X_val_encoded = dv.transform(val_dict)\n",
    "\n",
    "# Train logistic regression\n",
    "model = LogisticRegression(solver='lbfgs', C=1.0, max_iter=10000)\n",
    "model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Predict probabilities for AUC\n",
    "y_val_pred = model.predict_proba(X_val_encoded)[:, 1]\n",
    "\n",
    "# Compute AUC\n",
    "val_auc = roc_auc_score(y_val, y_val_pred)\n",
    "print(f\"Validation AUC: {val_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bddfc4-155b-4e9f-bd9c-9f21618a6265",
   "metadata": {},
   "source": [
    "### Question 3: Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "410f0847-86fa-4e5b-930d-42d2bafac829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision and Recall intersect at threshold: 0.550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\andre\\Projects\\machine_learning_zoomcamp\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "# Probabilities from your logistic regression model\n",
    "y_scores = y_val_pred\n",
    "\n",
    "# Define thresholds from 0.0 to 1.0 with step 0.01\n",
    "thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "\n",
    "# Compute precision and recall for each threshold\n",
    "for t in thresholds:\n",
    "    preds = (y_scores >= t).astype(int)\n",
    "    precisions.append(precision_score(y_val, preds))\n",
    "    recalls.append(recall_score(y_val, preds))\n",
    "\n",
    "# Convert to numpy arrays for easy computation\n",
    "precisions = np.array(precisions)\n",
    "recalls = np.array(recalls)\n",
    "\n",
    "# Find where precision and recall are approximately equal\n",
    "diff = np.abs(precisions - recalls)\n",
    "intersection_idx = np.argmin(diff)\n",
    "intersection_threshold = thresholds[intersection_idx]\n",
    "\n",
    "print(f\"Precision and Recall intersect at threshold: {intersection_threshold:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058720d5-56f4-4d21-aea4-6c9ae97351e5",
   "metadata": {},
   "source": [
    "### Question 4: F1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1295778e-ae15-4bdc-9115-6bbdea01f02c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 is maximal at threshold: 0.530 (F1 = 0.878)\n"
     ]
    }
   ],
   "source": [
    "thresholds = np.arange(0.0, 1.01, 0.01)\n",
    "f1_scores = []\n",
    "\n",
    "for t in thresholds:\n",
    "    preds = (y_scores >= t).astype(int)\n",
    "    p = precision_score(y_val, preds, zero_division=0)\n",
    "    r = recall_score(y_val, preds)\n",
    "    \n",
    "    if (p + r) == 0:\n",
    "        f1 = 0\n",
    "    else:\n",
    "        f1 = 2 * (p * r) / (p + r)\n",
    "    \n",
    "    f1_scores.append(f1)\n",
    "\n",
    "f1_scores = np.array(f1_scores)\n",
    "\n",
    "# Find threshold where F1 is maximal\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_threshold = thresholds[best_idx]\n",
    "best_f1 = f1_scores[best_idx]\n",
    "\n",
    "print(f\"F1 is maximal at threshold: {best_threshold:.3f} (F1 = {best_f1:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15062752-8b5c-4ea3-9af4-f0d2fb75c62c",
   "metadata": {},
   "source": [
    "### Question 5: 5-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfc373c2-b5b0-415e-9387-6848c2bb111b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC scores per fold: [0.923, 0.911, 0.935, 0.929, 0.915]\n",
      "Mean AUC: 0.923\n",
      "Standard deviation: 0.009\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "df_full_train = df.copy()\n",
    "\n",
    "X_full = df_full_train.drop(columns=['converted'])\n",
    "y_full = df_full_train['converted']\n",
    "\n",
    "# Ensure categorical columns are strings\n",
    "categorical = X_full.select_dtypes(include=['object']).columns.tolist()\n",
    "for col in categorical:\n",
    "    X_full[col] = X_full[col].astype(str)\n",
    "\n",
    "# Numerical columns\n",
    "numerical = X_full.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# Combine categorical + numerical\n",
    "features = categorical + numerical\n",
    "\n",
    "# Initialize KFold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_idx, val_idx in kfold.split(X_full):\n",
    "    X_train = X_full.iloc[train_idx]\n",
    "    X_val = X_full.iloc[val_idx]\n",
    "    y_train = y_full.iloc[train_idx]\n",
    "    y_val = y_full.iloc[val_idx]\n",
    "\n",
    "    # One-hot encoding with DictVectorizer\n",
    "    dv = DictVectorizer(sparse=False)\n",
    "    train_dict = X_train[features].to_dict(orient='records')\n",
    "    val_dict = X_val[features].to_dict(orient='records')\n",
    "\n",
    "    X_train_encoded = dv.fit_transform(train_dict)\n",
    "    X_val_encoded = dv.transform(val_dict)\n",
    "\n",
    "    # Train Logistic Regression\n",
    "    model = LogisticRegression(solver='lbfgs', C=1.0, max_iter=10000)\n",
    "    model.fit(X_train_encoded, y_train)\n",
    "\n",
    "    # Predict probabilities\n",
    "    y_val_pred = model.predict_proba(X_val_encoded)[:, 1]\n",
    "\n",
    "    # Compute AUC\n",
    "    auc = roc_auc_score(y_val, y_val_pred)\n",
    "    scores.append(auc)\n",
    "\n",
    "# Compute mean and std\n",
    "mean_auc = np.mean(scores)\n",
    "std_auc = np.std(scores)\n",
    "\n",
    "print(\"AUC scores per fold:\", [round(s, 3) for s in scores])\n",
    "print(f\"Mean AUC: {mean_auc:.3f}\")\n",
    "print(f\"Standard deviation: {std_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d004d9ce-5917-44cb-b5fd-acceec326672",
   "metadata": {},
   "source": [
    "### Question 6: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "df16fe4b-a679-4c57-a95a-038046401fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C=1e-06: mean AUC=0.55, std=0.031\n",
      "C=0.001: mean AUC=0.87, std=0.012\n",
      "C=1: mean AUC=0.923, std=0.009\n",
      "\n",
      "Best C: 1\n"
     ]
    }
   ],
   "source": [
    "# Prepare data\n",
    "df_full_train = df.copy()\n",
    "\n",
    "X_full = df_full_train.drop(columns=['converted'])\n",
    "y_full = df_full_train['converted']\n",
    "\n",
    "# Ensure categorical columns are strings\n",
    "categorical = X_full.select_dtypes(include=['object']).columns.tolist()\n",
    "for col in categorical:\n",
    "    X_full[col] = X_full[col].astype(str)\n",
    "\n",
    "numerical = X_full.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "features = categorical + numerical\n",
    "\n",
    "# Initialize KFold\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "# Values of C to test\n",
    "C_values = [0.000001, 0.001, 1]\n",
    "\n",
    "results = {}\n",
    "\n",
    "for C in C_values:\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kfold.split(X_full):\n",
    "        X_train = X_full.iloc[train_idx]\n",
    "        X_val = X_full.iloc[val_idx]\n",
    "        y_train = y_full.iloc[train_idx]\n",
    "        y_val = y_full.iloc[val_idx]\n",
    "\n",
    "        # One-hot encoding\n",
    "        dv = DictVectorizer(sparse=False)\n",
    "        train_dict = X_train[features].to_dict(orient='records')\n",
    "        val_dict = X_val[features].to_dict(orient='records')\n",
    "\n",
    "        X_train_encoded = dv.fit_transform(train_dict)\n",
    "        X_val_encoded = dv.transform(val_dict)\n",
    "\n",
    "        # Train logistic regression with specific C\n",
    "        model = LogisticRegression(solver='lbfgs', C=C, max_iter=10000)\n",
    "        model.fit(X_train_encoded, y_train)\n",
    "\n",
    "        # Predict and compute AUC\n",
    "        y_val_pred = model.predict_proba(X_val_encoded)[:, 1]\n",
    "        auc = roc_auc_score(y_val, y_val_pred)\n",
    "        scores.append(auc)\n",
    "\n",
    "    # Compute mean and std for this C\n",
    "    mean_auc = np.mean(scores)\n",
    "    std_auc = np.std(scores)\n",
    "\n",
    "    results[C] = (round(mean_auc, 3), round(std_auc, 3))\n",
    "\n",
    "# Print results\n",
    "for C, (mean_auc, std_auc) in results.items():\n",
    "    print(f\"C={C}: mean AUC={mean_auc}, std={std_auc}\")\n",
    "\n",
    "# Find best C\n",
    "best_C = max(results, key=lambda c: (results[c][0], -results[c][1], -c))\n",
    "print(f\"\\nBest C: {best_C}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880cb78-4e59-4cff-9823-7e11d9701aad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
